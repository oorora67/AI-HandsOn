{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oorora67/AI-HandsOn/blob/master/AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kT5PtucDNLoI",
        "colab_type": "text"
      },
      "source": [
        "# Google Drive　マウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A__YTxfFKmQH",
        "colab": {}
      },
      "source": [
        "#Google Drive　Mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7iXgdzFM7ok",
        "colab_type": "text"
      },
      "source": [
        "# **画像収集スクリプト**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnnW18bIdhbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import os\n",
        "import sys\n",
        "import urllib\n",
        "from IPython.display import Image,display_jpeg\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "#集めたい画像の名前を入力\n",
        "Search_Name = 'カレー'\n",
        "#集めたい枚数を入力\n",
        "Get_Number = 10\n",
        "\n",
        "class Google:\n",
        "    def __init__(self):\n",
        "        self.GOOGLE_SEARCH_URL = 'https://www.google.co.jp/search'\n",
        "        self.session = requests.session()\n",
        "        self.session.headers.update(\n",
        "            {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:10.0) Gecko/20100101 Firefox/10.0'})\n",
        "\n",
        "    def search(self, keyword, maximum):\n",
        "        print('begin searching', keyword)\n",
        "        query = self.query_gen(keyword)\n",
        "        return self.image_search(query, maximum)\n",
        "\n",
        "    def query_gen(self, keyword):\n",
        "        # search query generator\n",
        "        page = 0\n",
        "        while True:\n",
        "            params = urllib.parse.urlencode({\n",
        "                'q': keyword,\n",
        "                'tbm': 'isch',\n",
        "                'ijn': str(page)})\n",
        "\n",
        "            yield self.GOOGLE_SEARCH_URL + '?' + params\n",
        "            page += 1\n",
        "\n",
        "    def image_search(self, query_gen, maximum):\n",
        "        # search image\n",
        "        result = []\n",
        "        total = 0\n",
        "        while True:\n",
        "            # search\n",
        "            html = self.session.get(next(query_gen)).text\n",
        "            soup = BeautifulSoup(html, 'lxml')\n",
        "            elements = soup.select('.rg_meta.notranslate')\n",
        "            jsons = [json.loads(e.get_text()) for e in elements]\n",
        "            imageURLs = [js['ou'] for js in jsons]\n",
        "\n",
        "            # add search result\n",
        "            if not len(imageURLs):\n",
        "                print('-> no more images')\n",
        "                break\n",
        "            elif len(imageURLs) > maximum - total:\n",
        "                result += imageURLs[:maximum - total]\n",
        "                break\n",
        "            else:\n",
        "                result += imageURLs\n",
        "                total += len(imageURLs)\n",
        "\n",
        "        print('-> found', str(len(result)), 'images')\n",
        "        return result\n",
        "\n",
        "\n",
        "def main():\n",
        "    google = Google()\n",
        "    # save location\n",
        "    name = Search_Name\n",
        "    data_dir = '/content/gdrive/My Drive/AI-HandsOn/data/'\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "    os.makedirs(data_dir + name, exist_ok=True)\n",
        "\n",
        "        # search image\n",
        "    result = google.search(\n",
        "    name, maximum=Get_Number)\n",
        "\n",
        "        # download\n",
        "    download_error = []\n",
        "    for i in range(len(result)):\n",
        "        print('-> downloading image', str(i + 1).zfill(4))\n",
        "        try:\n",
        "            urllib.request.urlretrieve(result[i], data_dir + name + '/' + str(i + 1).zfill(4) + '.jpg')\n",
        "            display_jpeg(Image(data_dir + name + '/' + str(i + 1).zfill(4) + '.jpg'))\n",
        "            continue\n",
        "        except:\n",
        "            print('--> could not download image', str(i + 1).zfill(4))\n",
        "            download_error.append(i + 1)\n",
        "            continue\n",
        "\n",
        "    print('complete download')\n",
        "    print('├─ download', len(result)-len(download_error), 'images')\n",
        "    print('└─ could not download', len(download_error), 'images', download_error)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwcG32WXvpbE",
        "colab_type": "text"
      },
      "source": [
        "# **取得した画像から学習用データセットを作成する**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZluvRYfrv5wO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#データベース作成スクリプト\n",
        "# pngファイルを24ビット，RGBに変換してNumpyの配列形式で保存する\n",
        " \n",
        "#from sklearn import cross_validation\n",
        "#from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import os, sys, glob\n",
        "import numpy as np\n",
        " \n",
        "#データベースフォルダの場所 \n",
        "data_path =\"/content/gdrive/My Drive/AI-HandsOn/data\"\n",
        "#ラベル・フォルダネーム\n",
        "files = sorted(os.listdir(data_path))\n",
        "files_list = [f for f in files if os.path.isdir(os.path.join(data_path, f))]\n",
        "print(\"path\",data_path)\n",
        "print(\"List:\",files_list) \n",
        "categories = files_list\n",
        "# 変換したデータの保存先\n",
        "os.makedirs('/content/gdrive/My Drive/AI-HandsOn/data_set', exist_ok=True)\n",
        "npy_file = \"/content/gdrive/My Drive/AI-HandsOn/data_set/data_set.npy\"\n",
        " \n",
        "# ディレクトリ確認\n",
        "for s, t in enumerate(categories):\n",
        "    if not os.path.exists(data_path + \"/\" + t):\n",
        "        print(\"ディレクトリが存在しません：\", data_path + \"/\" + t)\n",
        "        quit()\n",
        " \n",
        "nb_classes = len(categories)\n",
        "image_size = 128  # 画像サイズを指定(一辺の長さが64)\n",
        "pixels = image_size * image_size * 3    # RGBだから3\n",
        " \n",
        "# 画像データを読み込んでNumpy配列に変換\n",
        "X = []  # 画像データ\n",
        "Y = []  # ラベルデータ\n",
        "for idx, cat in enumerate(categories):\n",
        "    label = [0 for i in range(nb_classes)]\n",
        "    label[idx] = 1\n",
        "    image_dir = data_path + \"/\" + cat\n",
        "    # glob関数を利用して拡張子が「.png」のものだけを列挙する\n",
        "    files = glob.glob(image_dir+\"/*.jpg\")\n",
        "    for i, f in enumerate(files):\n",
        "        img = Image.open(f)\n",
        "        img = img.convert(\"RGB\")\n",
        "        img = img.resize((image_size, image_size))\n",
        "        data = np.asarray(img)\n",
        "        X.append(data)\n",
        "        Y.append(label)\n",
        "        # 進捗表示\n",
        "        print(\"\\r\",cat, \":\", i // len(files) * 100,\"%\",end=\"\")\n",
        "    print(\"\\r\",cat, \": 100%\")\n",
        " \n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        " \n",
        "# 学習データとテストデータを分ける\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y)\n",
        "xy = (X_train, X_test, y_train, y_test)\n",
        "np.save(npy_file, xy)\n",
        " \n",
        "print(\"ok,\", len(Y), \",ファイル名：\", npy_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lHbZ8uY5uLR6"
      },
      "source": [
        "# **取得した画像を使って学習を行う**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asYDJTYvR_Sx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install numpy==1.16.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2jov7_Gt_50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.contrib.tpu.python.tpu import keras_support\n",
        "from tensorflow.keras.layers import Input, Conv2D,MaxPooling2D, BatchNormalization, Activation, AveragePooling2D, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import os\n",
        "# Kerasで構築したCNNモデルを用いて学習処理\n",
        "import numpy as np\n",
        " \n",
        "#データベースフォルダの場所 \n",
        "data_path =\"/content/gdrive/My Drive/AI-HandsOn/data\"\n",
        "#ラベル・フォルダネーム\n",
        "files = sorted(os.listdir(data_path))\n",
        "files_list = [f for f in files if os.path.isdir(os.path.join(data_path, f))]\n",
        "print(\"path\",data_path)\n",
        "print(\"List:\",files_list) \n",
        "categories = files_list\n",
        "\n",
        "os.makedirs('/content/gdrive/My Drive/AI-HandsOn/trained_data', exist_ok=True)\n",
        "hdf5_file = \"/content/gdrive/My Drive/AI-HandsOn/trained_data/trained_data.hdf5\"\n",
        "npy_file = \"/content/gdrive/My Drive/AI-HandsOn/data_set/data_set.npy\"\n",
        " \n",
        " \n",
        "nb_classes = len(categories)\n",
        "image_size = 128\n",
        "print('nb_classes',nb_classes)\n",
        "# 既に学習済みモデルがあればプログラムを終了する\n",
        "if os.path.exists(hdf5_file):\n",
        "    print(\"既に学習済みモデルが存在します：\", hdf5_file)\n",
        "    print(\"新たに学習を行う場合は，上記ファイル名を変更ないしは削除してください。\")\n",
        "    quit()\n",
        " \n",
        "# データをロード\n",
        "X_train, X_test, y_train, y_test = np.load(npy_file)\n",
        "# データを正規化する\n",
        "X_train = X_train.astype(\"float\") / 256\n",
        "X_test  = X_test.astype(\"float\")  / 256\n",
        "print('X_train shape:', X_train.shape)\n",
        " \n",
        "# CNNのモデルを構築\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3),padding = 'same',input_shape = X_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(nb_classes)) # 出力データのカテゴリー数を指定\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
        " \n",
        " \n",
        "# モデルを訓練する\n",
        "model.fit(X_train, y_train, epochs=40, batch_size=64)\n",
        " \n",
        "# モデルを保存\n",
        "model.save_weights(hdf5_file, save_format=\"h5\")\n",
        "print(\"新しい学習モデルを保存しました：\", hdf5_file)\n",
        " \n",
        "# モデルを評価する\n",
        "score = model.evaluate(X_test, y_test)\n",
        "print('loss=', score[0])\n",
        "print('accuracy=', score[1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6w-NC3rvALu",
        "colab_type": "text"
      },
      "source": [
        "# **学習した結果を使って画像を判定する**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqDKMQSXvJTM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}